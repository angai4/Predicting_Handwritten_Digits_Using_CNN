{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Original LeNet5 Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet5 architecture consists of two sequences of convolutional and average pooling layers that perform image processing. The last layer of the sequences is then flattened. Therefore, each neuron in the resulting series of convoluted 2-D arrays  is copied into a single line of neurons. Two fully connected layers and a softmax classifier complete the network and provide the output in terms of probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nbfAnboFaTyb",
    "outputId": "159fd9b6-c6a8-4a41-975c-823228167693"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Aaron\\anaconda3\\envs\\DL4Denv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Aaron\\anaconda3\\envs\\DL4Denv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Aaron\\anaconda3\\envs\\DL4Denv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Aaron\\anaconda3\\envs\\DL4Denv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Aaron\\anaconda3\\envs\\DL4Denv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Aaron\\anaconda3\\envs\\DL4Denv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary tools, you need to collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2y0VwkX0ac-e"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded data consists of single-channel 28-X-28 pixel images representing handwritten numbers from zero to nine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Np0al_jRae03",
    "outputId": "b8bc8987-27a9-4896-d291-cf9a9448f770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 => [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# transform targets into one-hot-encoded vectors\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(y_train[0], end=' => ')\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is 0 based and that the 1 appears at the position corresponding to the number 5. This setting is used because the neural network needs a response layer, which is a set of neurons that should become activated if the provided answer is correct. In this case, you see ten neurons, and in the training phase, the code activates the correct answer (the value at the correct position is set to 1) and turns the others off (their values are 0). In the test phase, the neural network uses its databases of examples to turn the correct neuron on, or at least more than the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XndplEL7ag0u",
    "outputId": "b6faa61d-bb85-47f9-8259-dbc398873194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# rescale 0-1 and cast training data as float32\n",
    "X_train = X_train.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "\n",
    "# reshape data to have also the channel dimension\n",
    "img_rows, img_cols = X_train.shape[1:]\n",
    "X_train = X_train.reshape(len(X_train), img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(len(X_test), img_rows, img_cols, 1)\n",
    "\n",
    "# notice the input shape\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel numbers, which range from 0 to 255, are transformed into a decimal value ranging from 0 to 1. The first two lines of code optimise the network to work properly with large numbers that could cause problems. The lines that follow reshape the images to have height, width, and channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wI6eDTO4a6b1"
   },
   "outputs": [],
   "source": [
    "# Call the sequential function that provides an empty model\n",
    "lenet = Sequential()\n",
    "\n",
    "# Convolutional Layer C1\n",
    "lenet.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', \n",
    "                 input_shape=input_shape, padding='same', name='C1'))\n",
    "\n",
    "# Pooling Layer S2\n",
    "lenet.add(AveragePooling2D(pool_size=(2, 2), name='S2'))\n",
    "\n",
    "# Convolutional Layer C3\n",
    "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', name='C3'))\n",
    "\n",
    "# Pooling Layer S4\n",
    "lenet.add(AveragePooling2D(pool_size=(2, 2), name='S4'))\n",
    "\n",
    "# Fully Connected Convolutional Layer C5\n",
    "lenet.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', name='C5'))\n",
    "lenet.add(Flatten())\n",
    "\n",
    "# Fully Connected Layer FC6\n",
    "lenet.add(Dense(84, activation='tanh', name='FC6'))\n",
    "\n",
    "#Output Layer (softmax activation)\n",
    "lenet.add(Dense(10, activation='softmax', name='OUTPUT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer added is a convolutional layer named C1. The convolution operates with a filter size of 6 and a kernel size of 5 X 5 pixels. **The activation function for all the layers of the network but the last one is *tanh***, a nonlinear function that was state of the art for activation at the Yann LeCun created LeNet5. It is outdated today, and should be replaced with a modern ReLU. Their is a pooling layer, named S2, which uses a 2 X 2-pixel kernel.\n",
    "\n",
    "The code proceeds with the sequences, always performed with a convolution and a pooling layer but this time using more filters.\n",
    "\n",
    "The LeNet5 closes incrementally using a convolution with 120 filters. This convolution does not have a pooling layer but rather a flattening layer, which projects the neurons into the last convolution layer as a dense layer. \n",
    "\n",
    "The closing of the network is a sequence of two dense layers that processes the convolution's outputs using the tanh and softmax activation. These two layers provide the final output layers where the neurons activate an output to signal the predicted answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "abwGHbsJa9iW",
    "outputId": "6fc5b0f2-52c3-4dae-d3e7-93ad15390f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "C1 (Conv2D)                  (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "S2 (AveragePooling2D)        (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "C3 (Conv2D)                  (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "S4 (AveragePooling2D)        (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "C5 (Conv2D)                  (None, 1, 1, 120)         48120     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "FC6 (Dense)                  (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "OUTPUT (Dense)               (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The network is now ready, so we get Keras to compile it.\n",
    "lenet.compile(loss=categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the network!\n",
    "\n",
    "Completing the run takes 50 epoch, each epoch processing batches of 64 images at one time (an epoch is the passing of the entire dataset through the neural network one time).\n",
    "\n",
    "The output will show a progress bar telling you the time to complete that epoch. You can also read the accuracy measures for both the training set (estimate of the goodness of the model) and the test set (the more realistic view). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1734
    },
    "colab_type": "code",
    "id": "-eoP-iCZa-YO",
    "outputId": "5ad2b7c5-3380-492f-88cf-6773b606ba4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 22s 360us/step - loss: 0.9289 - acc: 0.7695 - val_loss: 0.4167 - val_acc: 0.8895\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 21s 353us/step - loss: 0.3626 - acc: 0.8982 - val_loss: 0.3045 - val_acc: 0.9114\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 0.2880 - acc: 0.9160 - val_loss: 0.2520 - val_acc: 0.9254\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 0.2431 - acc: 0.9288 - val_loss: 0.2149 - val_acc: 0.9346\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 0.2081 - acc: 0.9389 - val_loss: 0.1864 - val_acc: 0.9442\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 21s 355us/step - loss: 0.1793 - acc: 0.9476 - val_loss: 0.1596 - val_acc: 0.9529\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.1560 - acc: 0.9548 - val_loss: 0.1396 - val_acc: 0.9590\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.1374 - acc: 0.9604 - val_loss: 0.1237 - val_acc: 0.9646\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 0.1225 - acc: 0.9645 - val_loss: 0.1112 - val_acc: 0.9691\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 0.1105 - acc: 0.9682 - val_loss: 0.1002 - val_acc: 0.9705\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 0.1009 - acc: 0.9708 - val_loss: 0.0911 - val_acc: 0.9741\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 22s 374us/step - loss: 0.0928 - acc: 0.9731 - val_loss: 0.0850 - val_acc: 0.9746\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0861 - acc: 0.9748 - val_loss: 0.0800 - val_acc: 0.9764\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 0.0803 - acc: 0.9765 - val_loss: 0.0751 - val_acc: 0.9777\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 0.0754 - acc: 0.9780 - val_loss: 0.0692 - val_acc: 0.9784\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.0713 - acc: 0.9791 - val_loss: 0.0670 - val_acc: 0.9794\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.0675 - acc: 0.9804 - val_loss: 0.0629 - val_acc: 0.9807\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 0.0640 - acc: 0.9809 - val_loss: 0.0608 - val_acc: 0.9818\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.0611 - acc: 0.9822 - val_loss: 0.0587 - val_acc: 0.9822\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 0.0585 - acc: 0.9828 - val_loss: 0.0559 - val_acc: 0.9828\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 0.0564 - acc: 0.9835 - val_loss: 0.0532 - val_acc: 0.9839\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 21s 351us/step - loss: 0.0541 - acc: 0.9838 - val_loss: 0.0538 - val_acc: 0.9835\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 21s 348us/step - loss: 0.0520 - acc: 0.9848 - val_loss: 0.0516 - val_acc: 0.9840\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 22s 359us/step - loss: 0.0502 - acc: 0.9850 - val_loss: 0.0504 - val_acc: 0.9842\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 21s 348us/step - loss: 0.0486 - acc: 0.9858 - val_loss: 0.0478 - val_acc: 0.9857\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 0.0472 - acc: 0.9862 - val_loss: 0.0465 - val_acc: 0.9859\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 22s 364us/step - loss: 0.0454 - acc: 0.9868 - val_loss: 0.0460 - val_acc: 0.9856\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 22s 375us/step - loss: 0.0443 - acc: 0.9871 - val_loss: 0.0453 - val_acc: 0.9858\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 27s 443us/step - loss: 0.0430 - acc: 0.9877 - val_loss: 0.0450 - val_acc: 0.9858\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 24s 407us/step - loss: 0.0418 - acc: 0.9880 - val_loss: 0.0442 - val_acc: 0.9863\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 26s 428us/step - loss: 0.0407 - acc: 0.9881 - val_loss: 0.0422 - val_acc: 0.9868\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 24s 395us/step - loss: 0.0397 - acc: 0.9887 - val_loss: 0.0422 - val_acc: 0.9861\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 23s 387us/step - loss: 0.0386 - acc: 0.9890 - val_loss: 0.0408 - val_acc: 0.9873\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 23s 385us/step - loss: 0.0375 - acc: 0.9892 - val_loss: 0.0403 - val_acc: 0.9873\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 23s 383us/step - loss: 0.0367 - acc: 0.9895 - val_loss: 0.0395 - val_acc: 0.9878\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 21s 358us/step - loss: 0.0359 - acc: 0.9895 - val_loss: 0.0394 - val_acc: 0.9875\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 21s 352us/step - loss: 0.0350 - acc: 0.9900 - val_loss: 0.0397 - val_acc: 0.9879\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 24s 408us/step - loss: 0.0343 - acc: 0.9902 - val_loss: 0.0386 - val_acc: 0.9876\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 23s 381us/step - loss: 0.0334 - acc: 0.9906 - val_loss: 0.0383 - val_acc: 0.9886\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 0.0326 - acc: 0.9908 - val_loss: 0.0374 - val_acc: 0.9878\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 22s 367us/step - loss: 0.0319 - acc: 0.9912 - val_loss: 0.0372 - val_acc: 0.9883\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 23s 386us/step - loss: 0.0313 - acc: 0.9911 - val_loss: 0.0391 - val_acc: 0.9883\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 23s 377us/step - loss: 0.0305 - acc: 0.9913 - val_loss: 0.0358 - val_acc: 0.9888\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 23s 382us/step - loss: 0.0301 - acc: 0.9915 - val_loss: 0.0367 - val_acc: 0.9882\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 0.0292 - acc: 0.9921 - val_loss: 0.0373 - val_acc: 0.9885\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 23s 385us/step - loss: 0.0288 - acc: 0.9920 - val_loss: 0.0349 - val_acc: 0.9888\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 24s 397us/step - loss: 0.0282 - acc: 0.9919 - val_loss: 0.0363 - val_acc: 0.9889\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.0275 - acc: 0.9925 - val_loss: 0.0343 - val_acc: 0.9890\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 24s 406us/step - loss: 0.0272 - acc: 0.9924 - val_loss: 0.0351 - val_acc: 0.9890\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 24s 398us/step - loss: 0.0265 - acc: 0.9929 - val_loss: 0.0348 - val_acc: 0.9890\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "history = lenet.fit(X_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, \n",
    "                                       y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet5 achieves an accuracy of **0.989**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LeNet5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
